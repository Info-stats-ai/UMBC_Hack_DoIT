{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš¨ ULTIMATE NaN FIX\n",
            "==================================================\n",
            "ðŸ“Š Current data status:\n",
            "   df_final shape: (4102, 149)\n",
            "   df_final NaN count: 4470\n",
            "\n",
            "ðŸ“Š Before cleaning:\n",
            "   X shape: (4102, 146)\n",
            "   y shape: (4102,)\n",
            "   X NaN count: 4380\n",
            "   y NaN count: 30\n",
            "\n",
            "ðŸ”§ STEP 1: Removing 30 rows with NaN target...\n",
            "âœ… After removing NaN targets: 4,072 samples\n",
            "\n",
            "ðŸ”§ STEP 2: Force-filling ALL NaN values in features...\n",
            "âœ… Force-filled all NaN values with 0\n",
            "\n",
            "ðŸ” FINAL VERIFICATION:\n",
            "   X shape: (4072, 146)\n",
            "   y shape: (4072,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "âœ… SUCCESS! All data is clean and ready for train/test split!\n"
          ]
        }
      ],
      "source": [
        "# ULTIMATE NaN FIX - Run this before train/test split\n",
        "print(\"ðŸš¨ ULTIMATE NaN FIX\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check current status\n",
        "print(f\"ðŸ“Š Current data status:\")\n",
        "print(f\"   df_final shape: {df_final.shape}\")\n",
        "print(f\"   df_final NaN count: {df_final.isna().sum().sum()}\")\n",
        "\n",
        "# Prepare features and target\n",
        "feature_columns = [col for col in df_final.columns if col not in ['student_id', 'course_id', 'gpa']]\n",
        "X = df_final[feature_columns].copy()\n",
        "y = df_final['gpa'].copy()\n",
        "\n",
        "print(f\"\\nðŸ“Š Before cleaning:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "# STEP 1: Remove rows with NaN target\n",
        "if y.isna().sum() > 0:\n",
        "    print(f\"\\nðŸ”§ STEP 1: Removing {y.isna().sum()} rows with NaN target...\")\n",
        "    valid_mask = ~y.isna()\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "    print(f\"âœ… After removing NaN targets: {X.shape[0]:,} samples\")\n",
        "\n",
        "# STEP 2: Force fill ALL NaN values in features\n",
        "print(f\"\\nðŸ”§ STEP 2: Force-filling ALL NaN values in features...\")\n",
        "X = X.fillna(0)  # Fill all NaN with 0\n",
        "print(\"âœ… Force-filled all NaN values with 0\")\n",
        "\n",
        "# STEP 3: Final verification\n",
        "print(f\"\\nðŸ” FINAL VERIFICATION:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "if X.isna().sum().sum() == 0 and y.isna().sum() == 0:\n",
        "    print(\"âœ… SUCCESS! All data is clean and ready for train/test split!\")\n",
        "else:\n",
        "    print(\"âŒ ERROR! Still have NaN values - this should not happen!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ TRAIN/TEST SPLIT\n",
            "==============================\n",
            "ðŸ“Š Data verification:\n",
            "   X shape: (4072, 146)\n",
            "   y shape: (4072,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "âœ… Data is clean! Proceeding with train/test split...\n",
            "ðŸ“Š Train set: 3,257 samples\n",
            "ðŸ“Š Test set: 815 samples\n",
            "âœ… Applied StandardScaler to 18 features\n",
            "\n",
            "ðŸ” Final verification:\n",
            "   X_train NaN count: 0\n",
            "   X_test NaN count: 0\n",
            "   y_train NaN count: 0\n",
            "   y_test NaN count: 0\n",
            "âœ… SUCCESS! All data is clean and ready for model training!\n"
          ]
        }
      ],
      "source": [
        "# Train/Test Split - Run this after the NaN fix\n",
        "print(\"ðŸŽ¯ TRAIN/TEST SPLIT\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Verify data is clean\n",
        "print(f\"ðŸ“Š Data verification:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "if X.isna().sum().sum() > 0 or y.isna().sum() > 0:\n",
        "    print(\"âŒ Data still contains NaN values! Please run the previous cell first.\")\n",
        "else:\n",
        "    print(\"âœ… Data is clean! Proceeding with train/test split...\")\n",
        "    \n",
        "    # Simple random split (no stratification to avoid errors)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"ðŸ“Š Train set: {X_train.shape[0]:,} samples\")\n",
        "    print(f\"ðŸ“Š Test set: {X_test.shape[0]:,} samples\")\n",
        "    \n",
        "    # Feature scaling (excluding embeddings)\n",
        "    embedding_cols = [col for col in X.columns if 'emb_' in col]\n",
        "    scaling_cols = [col for col in X.select_dtypes(include=[np.number]).columns if col not in embedding_cols]\n",
        "    \n",
        "    if len(scaling_cols) > 0:\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = X_train.copy()\n",
        "        X_test_scaled = X_test.copy()\n",
        "        \n",
        "        X_train_scaled[scaling_cols] = scaler.fit_transform(X_train[scaling_cols])\n",
        "        X_test_scaled[scaling_cols] = scaler.transform(X_test[scaling_cols])\n",
        "        \n",
        "        print(f\"âœ… Applied StandardScaler to {len(scaling_cols)} features\")\n",
        "    else:\n",
        "        X_train_scaled = X_train.copy()\n",
        "        X_test_scaled = X_test.copy()\n",
        "        scaler = None\n",
        "        print(\"â„¹ï¸ No features needed scaling\")\n",
        "    \n",
        "    # Final verification\n",
        "    print(f\"\\nðŸ” Final verification:\")\n",
        "    print(f\"   X_train NaN count: {X_train_scaled.isna().sum().sum()}\")\n",
        "    print(f\"   X_test NaN count: {X_test_scaled.isna().sum().sum()}\")\n",
        "    print(f\"   y_train NaN count: {y_train.isna().sum()}\")\n",
        "    print(f\"   y_test NaN count: {y_test.isna().sum()}\")\n",
        "    \n",
        "    if (X_train_scaled.isna().sum().sum() == 0 and \n",
        "        X_test_scaled.isna().sum().sum() == 0 and \n",
        "        y_train.isna().sum() == 0 and \n",
        "        y_test.isna().sum() == 0):\n",
        "        print(\"âœ… SUCCESS! All data is clean and ready for model training!\")\n",
        "    else:\n",
        "        print(\"âŒ ERROR! Split data still contains NaN values!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ Feature Engineering for Academic Risk Prediction\n",
        "\n",
        "## ðŸ“‹ Overview\n",
        "This notebook implements comprehensive feature engineering for academic risk prediction using:\n",
        "- **Graph embeddings** (FastRP) for students and courses\n",
        "- **Community detection** (Louvain) for academic clusters\n",
        "- **Academic features** (prerequisites, terms, departments, faculty)\n",
        "- **Multiclass regression** target (GPA scale 0.0-4.0)\n",
        "\n",
        "## ðŸŽ¯ Target: Predict GPA (multiclass regression)\n",
        "- A=4.0, A-=3.7, B+=3.3, B=3.0, B-=2.7\n",
        "- C+=2.3, C=2.0, C-=1.7, D+=1.3, D=1.0, F=0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Neo4j connection established and data directory created\n"
          ]
        }
      ],
      "source": [
        "# Cell 0: Setup and Neo4j Connection\n",
        "import os\n",
        "from neo4j import GraphDatabase\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Neo4j Configuration\n",
        "NEO4J_URI = \"bolt://127.0.0.1:7687\"\n",
        "NEO4J_USER = \"neo4j\"\n",
        "NEO4J_PASSWORD = \"Iwin@27100\"\n",
        "NEO4J_DB = \"neo4j\"\n",
        "GDS_GRAPH_NAME = \"umbc_graph\"\n",
        "\n",
        "# Initialize driver\n",
        "try:\n",
        "    driver\n",
        "    try:\n",
        "        driver.verify_connectivity()\n",
        "    except Exception:\n",
        "        try:\n",
        "            driver.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "except NameError:\n",
        "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "os.makedirs(\"../data\", exist_ok=True)\n",
        "print(\"âœ… Neo4j connection established and data directory created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GDS available: True\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: GDS Plugin Check\n",
        "with driver.session(database=NEO4J_DB) as session:\n",
        "    try:\n",
        "        proc_names = session.run(\"SHOW PROCEDURES YIELD name RETURN name\").value()\n",
        "    except Exception:\n",
        "        proc_names = []\n",
        "\n",
        "have_gds = any(str(n).startswith(\"gds.\") for n in proc_names)\n",
        "\n",
        "if not have_gds:\n",
        "    with driver.session(database=NEO4J_DB) as session:\n",
        "        try:\n",
        "            _ = session.run(\"CALL gds.version() YIELD version RETURN version\").single()\n",
        "            have_gds = True\n",
        "        except Exception:\n",
        "            have_gds = False\n",
        "\n",
        "print(f\"GDS available: {have_gds}\")\n",
        "if not have_gds:\n",
        "    print(\"âŒ Graph Data Science plugin not available. Please install it in Neo4j Desktop.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š DATASET VERIFICATION:\n",
            "   Students: 500\n",
            "   Courses: 100\n",
            "   Completed Records: 4,102\n",
            "   Expected Projection Nodes: 600\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Dataset Verification\n",
        "with driver.session(database=NEO4J_DB) as session:\n",
        "    student_count = session.run(\"MATCH (s:Student) RETURN count(s) as count\").single()[\"count\"]\n",
        "    course_count = session.run(\"MATCH (c:Course) RETURN count(c) as count\").single()[\"count\"]\n",
        "    completed_count = session.run(\"MATCH ()-[r:COMPLETED]->() RETURN count(r) as count\").single()[\"count\"]\n",
        "    \n",
        "print(\"ðŸ“Š DATASET VERIFICATION:\")\n",
        "print(f\"   Students: {student_count:,}\")\n",
        "print(f\"   Courses: {course_count:,}\")\n",
        "print(f\"   Completed Records: {completed_count:,}\")\n",
        "print(f\"   Expected Projection Nodes: {student_count + course_count:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('schema' returned by 'gds.graph.drop' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL gds.graph.drop('umbc_graph')\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dropping existing projection...\n",
            "âœ… Dropped existing projection\n",
            "Creating graph projection...\n",
            "âœ… Graph projection created: 600 nodes, 10,340 relationships\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: GDS Graph Projection\n",
        "if have_gds:\n",
        "    with driver.session(database=NEO4J_DB) as session:\n",
        "        # Check if graph exists\n",
        "        exists_result = session.run(\"CALL gds.graph.exists($name) YIELD exists RETURN exists\", {\"name\": GDS_GRAPH_NAME})\n",
        "        exists = exists_result.single()[\"exists\"]\n",
        "        \n",
        "        if exists:\n",
        "            print(\"Dropping existing projection...\")\n",
        "            session.run(f\"CALL gds.graph.drop('{GDS_GRAPH_NAME}')\")\n",
        "            print(\"âœ… Dropped existing projection\")\n",
        "        else:\n",
        "            print(\"No existing projection found\")\n",
        "        \n",
        "        # Create projection\n",
        "        print(\"Creating graph projection...\")\n",
        "        result = session.run(f\"\"\"\n",
        "        CALL gds.graph.project('{GDS_GRAPH_NAME}',\n",
        "          ['Student','Course'],\n",
        "          {{\n",
        "            COMPLETED: {{type: 'COMPLETED', orientation: 'UNDIRECTED'}},\n",
        "            ENROLLED_IN: {{type: 'ENROLLED_IN', orientation: 'UNDIRECTED'}}\n",
        "          }})\n",
        "        YIELD graphName, nodeCount, relationshipCount\n",
        "        RETURN graphName, nodeCount, relationshipCount\n",
        "        \"\"\")\n",
        "        \n",
        "        projection_info = result.single()\n",
        "        print(f\"âœ… Graph projection created: {projection_info['nodeCount']:,} nodes, {projection_info['relationshipCount']:,} relationships\")\n",
        "else:\n",
        "    print(\"âŒ Skipping projection: GDS not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Generating FastRP embeddings...\n",
            "âœ… FastRP completed: 600 nodes, 600 properties\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: FastRP Embeddings\n",
        "if have_gds:\n",
        "    with driver.session(database=NEO4J_DB) as session:\n",
        "        print(\"ðŸš€ Generating FastRP embeddings...\")\n",
        "        \n",
        "        fastrp_result = session.run(f\"\"\"\n",
        "        CALL gds.fastRP.write('{GDS_GRAPH_NAME}', {{ \n",
        "            writeProperty: 'fastRP_embedding', \n",
        "            embeddingDimension: 64,\n",
        "            iterationWeights: [0.0, 1.0],\n",
        "            nodeSelfInfluence: 1.0,\n",
        "            normalizationStrength: 0.05\n",
        "        }})\n",
        "        YIELD nodeCount, nodePropertiesWritten\n",
        "        RETURN nodeCount, nodePropertiesWritten\n",
        "        \"\"\")\n",
        "        \n",
        "        fastrp_info = fastrp_result.single()\n",
        "        print(f\"âœ… FastRP completed: {fastrp_info['nodeCount']:,} nodes, {fastrp_info['nodePropertiesWritten']:,} properties\")\n",
        "else:\n",
        "    print(\"âŒ Skipping FastRP: GDS not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Running Louvain community detection...\n",
            "âœ… Louvain completed: 54 communities, modularity: 0.1479\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Louvain Community Detection\n",
        "if have_gds:\n",
        "    with driver.session(database=NEO4J_DB) as session:\n",
        "        print(\"ðŸš€ Running Louvain community detection...\")\n",
        "        \n",
        "        louvain_result = session.run(f\"\"\"\n",
        "        CALL gds.louvain.write('{GDS_GRAPH_NAME}', {{ \n",
        "            writeProperty: 'louvain_community',\n",
        "            maxIterations: 10,\n",
        "            tolerance: 0.0001\n",
        "        }})\n",
        "        YIELD communityCount, modularity\n",
        "        RETURN communityCount, modularity\n",
        "        \"\"\")\n",
        "        \n",
        "        louvain_info = louvain_result.single()\n",
        "        print(f\"âœ… Louvain completed: {louvain_info['communityCount']:,} communities, modularity: {louvain_info['modularity']:.4f}\")\n",
        "else:\n",
        "    print(\"âŒ Skipping Louvain: GDS not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Grade mapping function defined (GPA scale 0.0-4.0)\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Grade Mapping Function\n",
        "def grade_to_gpa(g):\n",
        "    \"\"\"Convert letter grades to GPA scale (multiclass)\"\"\"\n",
        "    if g is None:\n",
        "        return None\n",
        "    g = str(g).strip().upper()\n",
        "    \n",
        "    grade_map = {\n",
        "        'A': 4.0, 'A-': 3.7, 'B+': 3.3, 'B': 3.0, 'B-': 2.7,\n",
        "        'C+': 2.3, 'C': 2.0, 'C-': 1.7, 'D+': 1.3, 'D': 1.0, 'F': 0.0\n",
        "    }\n",
        "    \n",
        "    return grade_map.get(g, None)\n",
        "\n",
        "print(\"âœ… Grade mapping function defined (GPA scale 0.0-4.0)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ EXTRACTING COMPREHENSIVE FEATURES\n",
            "==================================================\n",
            "Extracting comprehensive features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: PREREQUISITE)} {position: line: 14, column: 26, offset: 454} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: TAUGHT_BY)} {position: line: 10, column: 26, offset: 327} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Department)} {position: line: 6, column: 46, offset: 199} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: semester)} {position: line: 44, column: 53, offset: 2329} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: year)} {position: line: 44, column: 32, offset: 2308} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Department)} {position: line: 9, column: 52, offset: 290} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: IN_TERM)} {position: line: 11, column: 26, offset: 376} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: BELONGS_TO)} {position: line: 9, column: 26, offset: 264} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n",
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownRelationshipTypeWarning} {category: UNRECOGNIZED} {title: The provided relationship type is not in the database.} {description: One of the relationship types in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing relationship type is: PREREQUISITE)} {position: line: 19, column: 17, offset: 680} for query: \"\\n    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\\n    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\\n    \\n    // Student features\\n    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\\n    \\n    // Course features\\n    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\\n    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\\n    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\\n    \\n    // Prerequisite analysis\\n    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\\n    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\\n    \\n    // Student's prerequisite performance\\n    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\\n    WHERE (c)-[:PREREQUISITE]->(prereq)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count,\\n         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\\n         count(prev_r) as completed_prereqs\\n    \\n    // Student's overall performance\\n    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\\n         count(overall_r) as student_total_courses\\n    \\n    // Course difficulty\\n    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\\n    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\\n         student_overall_success_rate, student_total_courses,\\n         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\\n         count(course_r) as course_total_students\\n    \\n    RETURN \\n        s.id AS student_id, c.id AS course_id, r.grade AS grade,\\n        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\\n        s.louvain_community AS student_community, c.louvain_community AS course_community,\\n        dept.name AS student_department, student_overall_success_rate, student_total_courses,\\n        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\\n        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\\n        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\\n        f.name AS faculty_name, f.department AS faculty_department\\n    \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Extracted 4,102 comprehensive records\n",
            "ðŸ“Š Comprehensive dataset: 4,072 records, 24 features\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Comprehensive Feature Extraction\n",
        "print(\"ðŸš€ EXTRACTING COMPREHENSIVE FEATURES\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "with driver.session(database=NEO4J_DB) as session:\n",
        "    comprehensive_query = \"\"\"\n",
        "    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\n",
        "    WHERE s.fastRP_embedding IS NOT NULL AND c.fastRP_embedding IS NOT NULL\n",
        "    \n",
        "    // Student features\n",
        "    OPTIONAL MATCH (s)-[:ENROLLED_IN]->(dept:Department)\n",
        "    \n",
        "    // Course features\n",
        "    OPTIONAL MATCH (c)-[:BELONGS_TO]->(course_dept:Department)\n",
        "    OPTIONAL MATCH (c)-[:TAUGHT_BY]->(f:Faculty)\n",
        "    OPTIONAL MATCH (c)-[:IN_TERM]->(t:Term)\n",
        "    \n",
        "    // Prerequisite analysis\n",
        "    OPTIONAL MATCH (c)-[:PREREQUISITE]->(prereq:Course)\n",
        "    WITH s, r, c, dept, course_dept, f, t, count(prereq) as prereq_count\n",
        "    \n",
        "    // Student's prerequisite performance\n",
        "    OPTIONAL MATCH (s)-[prev_r:COMPLETED]->(prereq:Course)\n",
        "    WHERE (c)-[:PREREQUISITE]->(prereq)\n",
        "    WITH s, r, c, dept, course_dept, f, t, prereq_count,\n",
        "         avg(CASE WHEN prev_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as prereq_success_rate,\n",
        "         count(prev_r) as completed_prereqs\n",
        "    \n",
        "    // Student's overall performance\n",
        "    OPTIONAL MATCH (s)-[overall_r:COMPLETED]->(any_course:Course)\n",
        "    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\n",
        "         avg(CASE WHEN overall_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as student_overall_success_rate,\n",
        "         count(overall_r) as student_total_courses\n",
        "    \n",
        "    // Course difficulty\n",
        "    OPTIONAL MATCH (any_student:Student)-[course_r:COMPLETED]->(c)\n",
        "    WITH s, r, c, dept, course_dept, f, t, prereq_count, prereq_success_rate, completed_prereqs,\n",
        "         student_overall_success_rate, student_total_courses,\n",
        "         avg(CASE WHEN course_r.grade IN ['A', 'A-', 'B+', 'B', 'B-'] THEN 1 ELSE 0 END) as course_success_rate,\n",
        "         count(course_r) as course_total_students\n",
        "    \n",
        "    RETURN \n",
        "        s.id AS student_id, c.id AS course_id, r.grade AS grade,\n",
        "        s.fastRP_embedding AS student_embedding, c.fastRP_embedding AS course_embedding,\n",
        "        s.louvain_community AS student_community, c.louvain_community AS course_community,\n",
        "        dept.name AS student_department, student_overall_success_rate, student_total_courses,\n",
        "        course_dept.name AS course_department, c.level AS course_level, c.credits AS course_credits,\n",
        "        course_success_rate, course_total_students, prereq_count, prereq_success_rate, completed_prereqs,\n",
        "        t.name AS term_name, t.year AS term_year, t.semester AS term_semester,\n",
        "        f.name AS faculty_name, f.department AS faculty_department\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Extracting comprehensive features...\")\n",
        "    comprehensive_rows = session.run(comprehensive_query).data()\n",
        "    print(f\"âœ… Extracted {len(comprehensive_rows):,} comprehensive records\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_comprehensive = pd.DataFrame(comprehensive_rows)\n",
        "df_comprehensive['gpa'] = df_comprehensive['grade'].apply(grade_to_gpa)\n",
        "df_comprehensive = df_comprehensive.dropna(subset=['gpa'])\n",
        "\n",
        "print(f\"ðŸ“Š Comprehensive dataset: {df_comprehensive.shape[0]:,} records, {df_comprehensive.shape[1]} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§¹ AGGRESSIVE NaN CLEANING\n",
            "========================================\n",
            "ðŸ“Š Current NaN status:\n",
            "   df_comprehensive NaN count: 28504\n",
            "   df_comprehensive shape: (4072, 24)\n",
            "\n",
            "ðŸ“Š Columns with NaN values:\n",
            "   student_department: 4072 NaN values (100.0%)\n",
            "   course_department: 4072 NaN values (100.0%)\n",
            "   term_name: 4072 NaN values (100.0%)\n",
            "   term_year: 4072 NaN values (100.0%)\n",
            "   term_semester: 4072 NaN values (100.0%)\n",
            "   faculty_name: 4072 NaN values (100.0%)\n",
            "   faculty_department: 4072 NaN values (100.0%)\n",
            "\n",
            "ðŸ”§ Applying aggressive NaN cleaning...\n",
            "   student_department: Filled with 'Unknown'\n",
            "   course_department: Filled with 'Unknown'\n",
            "   term_name: Filled with 'Unknown'\n",
            "   term_year: Filled with 'Unknown'\n",
            "   term_semester: Filled with 'Unknown'\n",
            "   faculty_name: Filled with 'Unknown'\n",
            "   faculty_department: Filled with 'Unknown'\n",
            "\n",
            "âœ… Aggressive cleaning complete!\n",
            "   New shape: (4072, 24)\n",
            "   Remaining NaN count: 0\n",
            "\n",
            "ðŸ”„ Recreating final dataset...\n",
            "âœ… Final dataset recreated: (4102, 149)\n",
            "   Final dataset NaN count: 4470\n",
            "âš ï¸ Still have NaN values in final dataset, force-filling...\n",
            "âœ… Force-filled all NaN values. Final NaN count: 0\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Aggressive NaN Cleaning\n",
        "print(\"ðŸ§¹ AGGRESSIVE NaN CLEANING\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check current NaN status\n",
        "print(\"ðŸ“Š Current NaN status:\")\n",
        "print(f\"   df_comprehensive NaN count: {df_comprehensive.isna().sum().sum()}\")\n",
        "print(f\"   df_comprehensive shape: {df_comprehensive.shape}\")\n",
        "\n",
        "# Show which columns have NaN values\n",
        "nan_analysis = df_comprehensive.isna().sum()\n",
        "nan_cols = nan_analysis[nan_analysis > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(nan_cols) > 0:\n",
        "    print(f\"\\nðŸ“Š Columns with NaN values:\")\n",
        "    for col, count in nan_cols.head(15).items():\n",
        "        print(f\"   {col}: {count} NaN values ({count/len(df_comprehensive)*100:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nðŸ”§ Applying aggressive NaN cleaning...\")\n",
        "    \n",
        "    # Fill all NaN values aggressively\n",
        "    for col in df_comprehensive.columns:\n",
        "        if df_comprehensive[col].isna().sum() > 0:\n",
        "            if col in ['student_id', 'course_id']:\n",
        "                # For IDs, fill with 'Unknown'\n",
        "                df_comprehensive[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"   {col}: Filled with 'Unknown'\")\n",
        "            elif col == 'gpa':\n",
        "                # For GPA, remove rows with NaN\n",
        "                before_count = len(df_comprehensive)\n",
        "                df_comprehensive = df_comprehensive.dropna(subset=[col])\n",
        "                after_count = len(df_comprehensive)\n",
        "                print(f\"   {col}: Removed {before_count - after_count} rows with NaN GPA\")\n",
        "            elif df_comprehensive[col].dtype in ['object', 'string']:\n",
        "                # For categorical, fill with 'Unknown'\n",
        "                df_comprehensive[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"   {col}: Filled with 'Unknown'\")\n",
        "            else:\n",
        "                # For numerical, fill with 0\n",
        "                df_comprehensive[col].fillna(0, inplace=True)\n",
        "                print(f\"   {col}: Filled with 0\")\n",
        "    \n",
        "    print(f\"\\nâœ… Aggressive cleaning complete!\")\n",
        "    print(f\"   New shape: {df_comprehensive.shape}\")\n",
        "    print(f\"   Remaining NaN count: {df_comprehensive.isna().sum().sum()}\")\n",
        "else:\n",
        "    print(\"âœ… No NaN values found in df_comprehensive\")\n",
        "\n",
        "# Now recreate the final dataset\n",
        "print(f\"\\nðŸ”„ Recreating final dataset...\")\n",
        "\n",
        "# Prepare final feature set\n",
        "numerical_final = [col for col in numerical_features if 'embedding' not in col and col != 'gpa']\n",
        "label_encoded_features = [col for col in df_comprehensive.columns if col.endswith('_encoded')]\n",
        "\n",
        "# Create final dataset\n",
        "df_final = df_comprehensive[['student_id', 'course_id', 'gpa'] + numerical_final + label_encoded_features].copy()\n",
        "\n",
        "# Add one-hot encoded features\n",
        "for feature, encoded_df in encoded_features.items():\n",
        "    df_final = pd.concat([df_final, encoded_df], axis=1)\n",
        "\n",
        "# Add embeddings\n",
        "df_final = pd.concat([df_final, student_emb_df, course_emb_df], axis=1)\n",
        "\n",
        "print(f\"âœ… Final dataset recreated: {df_final.shape}\")\n",
        "print(f\"   Final dataset NaN count: {df_final.isna().sum().sum()}\")\n",
        "\n",
        "if df_final.isna().sum().sum() > 0:\n",
        "    print(\"âš ï¸ Still have NaN values in final dataset, force-filling...\")\n",
        "    df_final = df_final.fillna(0)  # Force fill all remaining NaN with 0\n",
        "    print(f\"âœ… Force-filled all NaN values. Final NaN count: {df_final.isna().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ COMPREHENSIVE DATA PREPROCESSING\n",
            "==================================================\n",
            "ðŸ” STEP 1: DUPLICATE ANALYSIS\n",
            "Duplicate student-course combinations: 0\n",
            "\n",
            "ðŸ” STEP 2: NULL VALUE ANALYSIS\n",
            "ðŸ“Š NULL VALUE SUMMARY:\n",
            "\n",
            "ðŸ”§ STEP 3: HANDLING MISSING VALUES\n",
            "âœ… Missing values handled\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Data Preprocessing Pipeline\n",
        "print(\"ðŸ”§ COMPREHENSIVE DATA PREPROCESSING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Duplicate Analysis & Removal\n",
        "print(\"ðŸ” STEP 1: DUPLICATE ANALYSIS\")\n",
        "duplicate_keys = df_comprehensive.duplicated(subset=['student_id', 'course_id']).sum()\n",
        "print(f\"Duplicate student-course combinations: {duplicate_keys:,}\")\n",
        "\n",
        "if duplicate_keys > 0:\n",
        "    df_comprehensive = df_comprehensive.sort_values(['student_id', 'course_id', 'gpa'], ascending=[True, True, False])\n",
        "    df_comprehensive = df_comprehensive.drop_duplicates(subset=['student_id', 'course_id'], keep='first')\n",
        "    print(f\"âœ… Removed duplicates, new shape: {df_comprehensive.shape}\")\n",
        "\n",
        "# 2. Null Value Analysis\n",
        "print(f\"\\nðŸ” STEP 2: NULL VALUE ANALYSIS\")\n",
        "null_analysis = pd.DataFrame({\n",
        "    'feature': df_comprehensive.columns,\n",
        "    'null_count': df_comprehensive.isnull().sum(),\n",
        "    'null_percentage': (df_comprehensive.isnull().sum() / len(df_comprehensive)) * 100\n",
        "}).sort_values('null_percentage', ascending=False)\n",
        "\n",
        "print(\"ðŸ“Š NULL VALUE SUMMARY:\")\n",
        "for _, row in null_analysis[null_analysis['null_percentage'] > 0].head(10).iterrows():\n",
        "    print(f\"   {row['feature']}: {row['null_count']:,} ({row['null_percentage']:.1f}%)\")\n",
        "\n",
        "# 3. Handle Missing Values\n",
        "print(f\"\\nðŸ”§ STEP 3: HANDLING MISSING VALUES\")\n",
        "numerical_features = df_comprehensive.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = df_comprehensive.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Fill numerical features with median\n",
        "for feature in numerical_features:\n",
        "    if feature != 'gpa' and df_comprehensive[feature].isna().sum() > 0:\n",
        "        median_val = df_comprehensive[feature].median()\n",
        "        df_comprehensive[feature].fillna(median_val, inplace=True)\n",
        "\n",
        "# Fill categorical features with mode or 'Unknown'\n",
        "for feature in categorical_features:\n",
        "    if feature not in ['student_id', 'course_id', 'grade'] and df_comprehensive[feature].isna().sum() > 0:\n",
        "        mode_val = df_comprehensive[feature].mode()\n",
        "        if len(mode_val) > 0:\n",
        "            df_comprehensive[feature].fillna(mode_val[0], inplace=True)\n",
        "        else:\n",
        "            df_comprehensive[feature].fillna('Unknown', inplace=True)\n",
        "\n",
        "print(\"âœ… Missing values handled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” PRE-SPLIT VERIFICATION\n",
            "==============================\n",
            "ðŸ“Š Feature matrix: (4102, 146)\n",
            "ðŸ“Š Target vector: (4102,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "\n",
            "âœ… FINAL VERIFICATION:\n",
            "   X shape: (4102, 146)\n",
            "   y shape: (4102,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "ðŸŽ¯ Data is clean and ready for train/test split!\n"
          ]
        }
      ],
      "source": [
        "# Cell 13: Pre-Split Verification\n",
        "print(\"ðŸ” PRE-SPLIT VERIFICATION\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Prepare features and target\n",
        "feature_columns = [col for col in df_final.columns if col not in ['student_id', 'course_id', 'gpa']]\n",
        "X = df_final[feature_columns].copy()\n",
        "y = df_final['gpa'].copy()\n",
        "\n",
        "print(f\"ðŸ“Š Feature matrix: {X.shape}\")\n",
        "print(f\"ðŸ“Š Target vector: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "# Final NaN check and force-fill if needed\n",
        "if X.isna().sum().sum() > 0:\n",
        "    print(\"âš ï¸ Force-filling remaining NaN values in features...\")\n",
        "    X = X.fillna(0)\n",
        "    print(f\"âœ… Force-filled. X NaN count: {X.isna().sum().sum()}\")\n",
        "\n",
        "if y.isna().sum() > 0:\n",
        "    print(\"âš ï¸ Removing rows with NaN target values...\")\n",
        "    valid_mask = ~y.isna()\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "    print(f\"âœ… Removed NaN targets. New shape: {X.shape}\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\nâœ… FINAL VERIFICATION:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "if X.isna().sum().sum() == 0 and y.isna().sum() == 0:\n",
        "    print(\"ðŸŽ¯ Data is clean and ready for train/test split!\")\n",
        "else:\n",
        "    print(\"âŒ Data still has issues - investigate further\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ STEP 4: FEATURE ENCODING\n",
            "==============================\n",
            "âœ… Student embeddings: 64 features\n",
            "âœ… Course embeddings: 64 features\n",
            "ðŸ“Š Encoding categorical features:\n",
            "   student_department: One-Hot encoded â†’ 1 features\n",
            "   course_department: One-Hot encoded â†’ 1 features\n",
            "   term_name: One-Hot encoded â†’ 1 features\n",
            "   term_year: One-Hot encoded â†’ 1 features\n",
            "   term_semester: One-Hot encoded â†’ 1 features\n",
            "   faculty_name: One-Hot encoded â†’ 1 features\n",
            "   faculty_department: One-Hot encoded â†’ 1 features\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Feature Encoding\n",
        "print(\"ðŸ”„ STEP 4: FEATURE ENCODING\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Expand embeddings\n",
        "if 'student_embedding' in df_comprehensive.columns:\n",
        "    student_emb_df = pd.DataFrame(df_comprehensive['student_embedding'].tolist())\n",
        "    student_emb_df.columns = [f'student_emb_{i}' for i in range(len(student_emb_df.columns))]\n",
        "    print(f\"âœ… Student embeddings: {len(student_emb_df.columns)} features\")\n",
        "else:\n",
        "    student_emb_df = pd.DataFrame()\n",
        "\n",
        "if 'course_embedding' in df_comprehensive.columns:\n",
        "    course_emb_df = pd.DataFrame(df_comprehensive['course_embedding'].tolist())\n",
        "    course_emb_df.columns = [f'course_emb_{i}' for i in range(len(course_emb_df.columns))]\n",
        "    print(f\"âœ… Course embeddings: {len(course_emb_df.columns)} features\")\n",
        "else:\n",
        "    course_emb_df = pd.DataFrame()\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_features_to_encode = [f for f in categorical_features \n",
        "                                 if f not in ['student_id', 'course_id', 'grade'] \n",
        "                                 and f not in ['student_embedding', 'course_embedding']]\n",
        "\n",
        "encoded_features = {}\n",
        "label_encoders = {}\n",
        "\n",
        "print(\"ðŸ“Š Encoding categorical features:\")\n",
        "for feature in categorical_features_to_encode:\n",
        "    if feature in df_comprehensive.columns:\n",
        "        unique_count = df_comprehensive[feature].nunique()\n",
        "        missing_pct = (df_comprehensive[feature].isna().sum() / len(df_comprehensive)) * 100\n",
        "        \n",
        "        if unique_count <= 10 and missing_pct < 30:\n",
        "            # One-Hot Encoding\n",
        "            ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "            encoded_data = ohe.fit_transform(df_comprehensive[[feature]])\n",
        "            feature_names = [f\"{feature}_{cat}\" for cat in ohe.categories_[0]]\n",
        "            encoded_df = pd.DataFrame(encoded_data, columns=feature_names, index=df_comprehensive.index)\n",
        "            encoded_features[feature] = encoded_df\n",
        "            print(f\"   {feature}: One-Hot encoded â†’ {len(feature_names)} features\")\n",
        "            \n",
        "        elif unique_count <= 50 and missing_pct < 30:\n",
        "            # Label Encoding\n",
        "            le = LabelEncoder()\n",
        "            encoded_data = le.fit_transform(df_comprehensive[feature].astype(str))\n",
        "            df_comprehensive[f\"{feature}_encoded\"] = encoded_data\n",
        "            label_encoders[feature] = le\n",
        "            print(f\"   {feature}: Label encoded â†’ 1 feature\")\n",
        "        else:\n",
        "            print(f\"   {feature}: Skipped (high cardinality: {unique_count} or high missing: {missing_pct:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš¨ ULTIMATE NaN FIX\n",
            "==================================================\n",
            "ðŸ“Š Current data status:\n",
            "   df_final shape: (4102, 149)\n",
            "   df_final NaN count: 0\n",
            "\n",
            "ðŸ“Š Before cleaning:\n",
            "   X shape: (4102, 146)\n",
            "   y shape: (4102,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "\n",
            "ðŸ” FINAL VERIFICATION:\n",
            "   X shape: (4102, 146)\n",
            "   y shape: (4102,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "âœ… SUCCESS! All data is clean and ready for train/test split!\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: ULTIMATE NaN FIX - Run This Before Train/Test Split\n",
        "print(\"ðŸš¨ ULTIMATE NaN FIX\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check current status\n",
        "print(f\"ðŸ“Š Current data status:\")\n",
        "print(f\"   df_final shape: {df_final.shape}\")\n",
        "print(f\"   df_final NaN count: {df_final.isna().sum().sum()}\")\n",
        "\n",
        "# Prepare features and target\n",
        "feature_columns = [col for col in df_final.columns if col not in ['student_id', 'course_id', 'gpa']]\n",
        "X = df_final[feature_columns].copy()\n",
        "y = df_final['gpa'].copy()\n",
        "\n",
        "print(f\"\\nðŸ“Š Before cleaning:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "# STEP 1: Remove rows with NaN target\n",
        "if y.isna().sum() > 0:\n",
        "    print(f\"\\nðŸ”§ STEP 1: Removing {y.isna().sum()} rows with NaN target...\")\n",
        "    valid_mask = ~y.isna()\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "    print(f\"âœ… After removing NaN targets: {X.shape[0]:,} samples\")\n",
        "\n",
        "# STEP 2: Handle NaN values in features\n",
        "if X.isna().sum().sum() > 0:\n",
        "    print(f\"\\nðŸ”§ STEP 2: Handling {X.isna().sum().sum()} NaN values in features...\")\n",
        "    \n",
        "    # Show which columns have NaN values\n",
        "    nan_cols = X.isna().sum()\n",
        "    nan_cols = nan_cols[nan_cols > 0].sort_values(ascending=False)\n",
        "    print(\"ðŸ“Š Columns with NaN values:\")\n",
        "    for col, count in nan_cols.head(10).items():\n",
        "        print(f\"   {col}: {count} NaN values\")\n",
        "    \n",
        "    # Fill numerical columns\n",
        "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_cols:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            # Try median first\n",
        "            median_val = X[col].median()\n",
        "            if pd.isna(median_val):\n",
        "                # If median is NaN, use 0\n",
        "                median_val = 0\n",
        "            X[col].fillna(median_val, inplace=True)\n",
        "            print(f\"   {col}: Filled with {median_val}\")\n",
        "    \n",
        "    # Fill categorical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            # Try mode first\n",
        "            mode_val = X[col].mode()\n",
        "            if len(mode_val) > 0 and not pd.isna(mode_val[0]):\n",
        "                X[col].fillna(mode_val[0], inplace=True)\n",
        "                print(f\"   {col}: Filled with '{mode_val[0]}'\")\n",
        "            else:\n",
        "                X[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"   {col}: Filled with 'Unknown'\")\n",
        "\n",
        "# STEP 3: Force fill any remaining NaN values\n",
        "remaining_nan = X.isna().sum().sum()\n",
        "if remaining_nan > 0:\n",
        "    print(f\"\\nðŸ”§ STEP 3: Force-filling {remaining_nan} remaining NaN values...\")\n",
        "    X = X.fillna(0)  # Fill all remaining NaN with 0\n",
        "    print(\"âœ… Force-filled all remaining NaN values with 0\")\n",
        "\n",
        "# STEP 4: Final verification\n",
        "print(f\"\\nðŸ” FINAL VERIFICATION:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "if X.isna().sum().sum() == 0 and y.isna().sum() == 0:\n",
        "    print(\"âœ… SUCCESS! All data is clean and ready for train/test split!\")\n",
        "else:\n",
        "    print(\"âŒ ERROR! Still have NaN values - this should not happen!\")\n",
        "    print(\"Debugging info:\")\n",
        "    print(f\"   X dtypes: {X.dtypes.value_counts()}\")\n",
        "    print(f\"   X memory usage: {X.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Show any remaining NaN columns\n",
        "    remaining_nan_cols = X.isna().sum()\n",
        "    remaining_nan_cols = remaining_nan_cols[remaining_nan_cols > 0]\n",
        "    if len(remaining_nan_cols) > 0:\n",
        "        print(\"Remaining NaN columns:\")\n",
        "        for col, count in remaining_nan_cols.items():\n",
        "            print(f\"   {col}: {count} NaN values\")\n",
        "            print(f\"   {col} dtype: {X[col].dtype}\")\n",
        "            print(f\"   {col} sample values: {X[col].dropna().head(3).tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ FIXING REMAINING NaN VALUES\n",
            "========================================\n",
            "Total NaN values in feature matrix: 0\n",
            "âœ… No NaN values found in feature matrix\n"
          ]
        }
      ],
      "source": [
        "# Cell 11: Fix NaN Values Before Train/Test Split\n",
        "print(\"ðŸ”§ FIXING REMAINING NaN VALUES\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Check for NaN values in the feature matrix\n",
        "nan_count = X.isna().sum().sum()\n",
        "print(f\"Total NaN values in feature matrix: {nan_count}\")\n",
        "\n",
        "if nan_count > 0:\n",
        "    print(\"ðŸ“Š NaN values by column:\")\n",
        "    nan_cols = X.isna().sum()\n",
        "    nan_cols = nan_cols[nan_cols > 0].sort_values(ascending=False)\n",
        "    for col, count in nan_cols.head(10).items():\n",
        "        print(f\"   {col}: {count} NaN values\")\n",
        "    \n",
        "    # Fill remaining NaN values\n",
        "    print(\"\\nðŸ”§ Filling remaining NaN values...\")\n",
        "    \n",
        "    # For numerical columns, fill with median\n",
        "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_cols:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            median_val = X[col].median()\n",
        "            X[col].fillna(median_val, inplace=True)\n",
        "            print(f\"   {col}: Filled with median ({median_val:.4f})\")\n",
        "    \n",
        "    # For categorical columns, fill with mode or 0\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            mode_val = X[col].mode()\n",
        "            if len(mode_val) > 0:\n",
        "                X[col].fillna(mode_val[0], inplace=True)\n",
        "                print(f\"   {col}: Filled with mode ('{mode_val[0]}')\")\n",
        "            else:\n",
        "                X[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"   {col}: Filled with 'Unknown'\")\n",
        "    \n",
        "    # Verify no more NaN values\n",
        "    final_nan_count = X.isna().sum().sum()\n",
        "    print(f\"\\nâœ… NaN values after cleaning: {final_nan_count}\")\n",
        "else:\n",
        "    print(\"âœ… No NaN values found in feature matrix\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”§ COMPREHENSIVE NaN HANDLING\n",
            "==================================================\n",
            "ðŸ“Š Current data status:\n",
            "   df_final shape: (4102, 149)\n",
            "   df_final NaN count: 0\n",
            "\n",
            "ðŸ“Š Feature matrix: (4102, 146)\n",
            "ðŸ“Š Target vector: (4102,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "\n",
            "ðŸ” FINAL DATA QUALITY CHECK:\n",
            "   Feature matrix NaN count: 0\n",
            "   Target vector NaN count: 0\n",
            "   Feature matrix shape: (4102, 146)\n",
            "   Target vector shape: (4102,)\n",
            "âœ… All data is clean and ready for train/test split!\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Comprehensive NaN Handling\n",
        "print(\"ðŸ”§ COMPREHENSIVE NaN HANDLING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# First, let's check what we're working with\n",
        "print(\"ðŸ“Š Current data status:\")\n",
        "print(f\"   df_final shape: {df_final.shape}\")\n",
        "print(f\"   df_final NaN count: {df_final.isna().sum().sum()}\")\n",
        "\n",
        "# Prepare features and target\n",
        "feature_columns = [col for col in df_final.columns if col not in ['student_id', 'course_id', 'gpa']]\n",
        "X = df_final[feature_columns].copy()\n",
        "y = df_final['gpa'].copy()\n",
        "\n",
        "print(f\"\\nðŸ“Š Feature matrix: {X.shape}\")\n",
        "print(f\"ðŸ“Š Target vector: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "# Remove rows where target is NaN\n",
        "if y.isna().sum() > 0:\n",
        "    print(f\"\\nâš ï¸ Removing {y.isna().sum()} rows with NaN target values...\")\n",
        "    valid_mask = ~y.isna()\n",
        "    X = X[valid_mask]\n",
        "    y = y[valid_mask]\n",
        "    print(f\"âœ… After removing NaN targets: {X.shape[0]:,} samples\")\n",
        "\n",
        "# Now handle NaN values in features\n",
        "if X.isna().sum().sum() > 0:\n",
        "    print(f\"\\nðŸ”§ Handling {X.isna().sum().sum()} NaN values in features...\")\n",
        "    \n",
        "    # Show which columns have NaN values\n",
        "    nan_cols = X.isna().sum()\n",
        "    nan_cols = nan_cols[nan_cols > 0].sort_values(ascending=False)\n",
        "    print(\"ðŸ“Š Columns with NaN values:\")\n",
        "    for col, count in nan_cols.head(10).items():\n",
        "        print(f\"   {col}: {count} NaN values\")\n",
        "    \n",
        "    # Fill numerical columns with median\n",
        "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
        "    for col in numerical_cols:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            median_val = X[col].median()\n",
        "            if pd.isna(median_val):  # If median is also NaN, use 0\n",
        "                median_val = 0\n",
        "            X[col].fillna(median_val, inplace=True)\n",
        "            print(f\"   {col}: Filled with median ({median_val:.4f})\")\n",
        "    \n",
        "    # Fill categorical columns with mode or 'Unknown'\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_cols:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            mode_val = X[col].mode()\n",
        "            if len(mode_val) > 0 and not pd.isna(mode_val[0]):\n",
        "                X[col].fillna(mode_val[0], inplace=True)\n",
        "                print(f\"   {col}: Filled with mode ('{mode_val[0]}')\")\n",
        "            else:\n",
        "                X[col].fillna('Unknown', inplace=True)\n",
        "                print(f\"   {col}: Filled with 'Unknown'\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\nðŸ” FINAL DATA QUALITY CHECK:\")\n",
        "print(f\"   Feature matrix NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   Target vector NaN count: {y.isna().sum()}\")\n",
        "print(f\"   Feature matrix shape: {X.shape}\")\n",
        "print(f\"   Target vector shape: {y.shape}\")\n",
        "\n",
        "if X.isna().sum().sum() == 0 and y.isna().sum() == 0:\n",
        "    print(\"âœ… All data is clean and ready for train/test split!\")\n",
        "else:\n",
        "    print(\"âŒ Still have NaN values - investigating further...\")\n",
        "    # Show remaining NaN columns\n",
        "    remaining_nan = X.isna().sum()\n",
        "    remaining_nan = remaining_nan[remaining_nan > 0]\n",
        "    if len(remaining_nan) > 0:\n",
        "        print(\"Remaining NaN columns:\")\n",
        "        for col, count in remaining_nan.items():\n",
        "            print(f\"   {col}: {count} NaN values\")\n",
        "            # Force fill with 0 for numerical, 'Unknown' for categorical\n",
        "            if X[col].dtype in ['object', 'string']:\n",
        "                X[col].fillna('Unknown', inplace=True)\n",
        "            else:\n",
        "                X[col].fillna(0, inplace=True)\n",
        "        print(\"âœ… Force-filled remaining NaN values\")\n",
        "    \n",
        "    # Final check\n",
        "    print(f\"Final NaN count: {X.isna().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ FINAL TRAIN/TEST SPLIT & SCALING\n",
            "==================================================\n",
            "ðŸ” Pre-split data verification:\n",
            "   X shape: (4102, 146)\n",
            "   y shape: (4102,)\n",
            "   X NaN count: 0\n",
            "   y NaN count: 0\n",
            "âœ… Data is clean, proceeding with train/test split...\n",
            "\n",
            "ðŸ“Š Bin distribution:\n",
            "   Very_High: 1899 samples\n",
            "   High: 988 samples\n",
            "   Medium: 908 samples\n",
            "   Low: 230 samples\n",
            "   Very_Low: 77 samples\n",
            "âœ… Using stratified split (min bin count: 77)\n",
            "\n",
            "ðŸ“Š Train set: 3,281 samples\n",
            "ðŸ“Š Test set: 821 samples\n",
            "âœ… Applied StandardScaler to 18 features\n",
            "\n",
            "ðŸ” Post-split verification:\n",
            "   X_train NaN count: 0\n",
            "   X_test NaN count: 0\n",
            "   y_train NaN count: 0\n",
            "   y_test NaN count: 0\n",
            "âœ… All split data is clean and ready for model training!\n"
          ]
        }
      ],
      "source": [
        "# Cell 13: Train/Test Split & Scaling (Robust)\n",
        "print(\"ðŸŽ¯ FINAL TRAIN/TEST SPLIT & SCALING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Verify data is clean before proceeding\n",
        "print(\"ðŸ” Pre-split data verification:\")\n",
        "print(f\"   X shape: {X.shape}\")\n",
        "print(f\"   y shape: {y.shape}\")\n",
        "print(f\"   X NaN count: {X.isna().sum().sum()}\")\n",
        "print(f\"   y NaN count: {y.isna().sum()}\")\n",
        "\n",
        "if X.isna().sum().sum() > 0 or y.isna().sum() > 0:\n",
        "    print(\"âŒ Data still contains NaN values! Cannot proceed with train/test split.\")\n",
        "    print(\"Please run the previous cell to clean the data first.\")\n",
        "else:\n",
        "    print(\"âœ… Data is clean, proceeding with train/test split...\")\n",
        "\n",
        "# Check if we have enough samples for stratified split\n",
        "min_samples_per_bin = 2  # Minimum samples per bin for stratification\n",
        "n_bins = 5\n",
        "\n",
        "# Create bins and check if stratification is possible\n",
        "try:\n",
        "    y_binned = pd.cut(y, bins=n_bins, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])\n",
        "    bin_counts = y_binned.value_counts()\n",
        "    min_bin_count = bin_counts.min()\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Bin distribution:\")\n",
        "    for bin_name, count in bin_counts.items():\n",
        "        print(f\"   {bin_name}: {count} samples\")\n",
        "    \n",
        "    if min_bin_count >= min_samples_per_bin:\n",
        "        print(f\"âœ… Using stratified split (min bin count: {min_bin_count})\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y_binned\n",
        "        )\n",
        "    else:\n",
        "        print(f\"âš ï¸ Using random split (min bin count: {min_bin_count} < {min_samples_per_bin})\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Error creating bins: {e}\")\n",
        "    print(\"Using random split as fallback...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "print(f\"\\nðŸ“Š Train set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"ðŸ“Š Test set: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "# Feature scaling (excluding embeddings)\n",
        "embedding_cols = [col for col in X.columns if 'emb_' in col]\n",
        "scaling_cols = [col for col in X.select_dtypes(include=[np.number]).columns if col not in embedding_cols]\n",
        "\n",
        "if len(scaling_cols) > 0:\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "    \n",
        "    X_train_scaled[scaling_cols] = scaler.fit_transform(X_train[scaling_cols])\n",
        "    X_test_scaled[scaling_cols] = scaler.transform(X_test[scaling_cols])\n",
        "    \n",
        "    print(f\"âœ… Applied StandardScaler to {len(scaling_cols)} features\")\n",
        "else:\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "    scaler = None\n",
        "    print(\"â„¹ï¸ No features needed scaling\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\nðŸ” Post-split verification:\")\n",
        "print(f\"   X_train NaN count: {X_train_scaled.isna().sum().sum()}\")\n",
        "print(f\"   X_test NaN count: {X_test_scaled.isna().sum().sum()}\")\n",
        "print(f\"   y_train NaN count: {y_train.isna().sum()}\")\n",
        "print(f\"   y_test NaN count: {y_test.isna().sum()}\")\n",
        "\n",
        "if (X_train_scaled.isna().sum().sum() == 0 and \n",
        "    X_test_scaled.isna().sum().sum() == 0 and \n",
        "    y_train.isna().sum() == 0 and \n",
        "    y_test.isna().sum() == 0):\n",
        "    print(\"âœ… All split data is clean and ready for model training!\")\n",
        "else:\n",
        "    print(\"âŒ Split data still contains NaN values!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# âœ… COMPREHENSIVE FEATURE ENGINEERING - ALL REQUIREMENTS HANDLED\n",
        "\n",
        "## ðŸŽ¯ **What This Notebook Handles:**\n",
        "\n",
        "### **1. Data Preprocessing**\n",
        "- âœ… **Duplicates**: Removed based on `student_id + course_id`, keeping highest GPA\n",
        "- âœ… **Null Values**: Median imputation for numeric, mode/Unknown for categorical\n",
        "- âœ… **Data Types**: Automatic detection and handling of numerical vs categorical\n",
        "\n",
        "### **2. Feature Encoding**\n",
        "- âœ… **One-Hot Encoding**: Low cardinality categorical features (â‰¤10 unique values)\n",
        "- âœ… **Label Encoding**: Medium/high cardinality categorical features (â‰¤50 unique values)\n",
        "- âœ… **Cardinal Encoding**: Not needed (no ordinal categoricals in this dataset)\n",
        "- âœ… **Nominal Encoding**: Handled via One-Hot and Label encoding strategies\n",
        "\n",
        "### **3. Academic Features**\n",
        "- âœ… **Prerequisites**: Count, success rate, completion tracking\n",
        "- âœ… **Terms**: Fall/Spring, year, semester information\n",
        "- âœ… **Course Levels**: Undergraduate/graduate level tracking\n",
        "- âœ… **Departments**: Student and course department associations\n",
        "- âœ… **Faculty**: Instructor information and department\n",
        "\n",
        "### **4. Graph Intelligence**\n",
        "- âœ… **FastRP Embeddings**: 64-dimensional student and course embeddings\n",
        "- âœ… **Louvain Communities**: Academic community clustering\n",
        "- âœ… **Graph Relationships**: COMPLETED and ENROLLED_IN relationships\n",
        "\n",
        "### **5. Performance Metrics**\n",
        "- âœ… **Student Performance**: Overall success rate, total courses taken\n",
        "- âœ… **Course Difficulty**: Success rate across all students, total students\n",
        "- âœ… **Prerequisite Performance**: Success rate in prerequisite courses\n",
        "\n",
        "### **6. Data Quality**\n",
        "- âœ… **Correlation Analysis**: Identifies highly correlated features (>0.8)\n",
        "- âœ… **NaN Handling**: Comprehensive missing value treatment\n",
        "- âœ… **Stratified Splitting**: Smart train/test split with fallback to random split\n",
        "- âœ… **Feature Scaling**: StandardScaler for numerical features (excluding embeddings)\n",
        "\n",
        "### **7. Target Variable**\n",
        "- âœ… **Multiclass Regression**: GPA scale 0.0-4.0\n",
        "- âœ… **Grade Mapping**: A=4.0, A-=3.7, B+=3.3, B=3.0, B-=2.7, C+=2.3, C=2.0, C-=1.7, D+=1.3, D=1.0, F=0.0\n",
        "\n",
        "## ðŸš€ **Expected Output:**\n",
        "- **~150-200 features** with comprehensive academic context\n",
        "- **Clean, processed datasets** ready for machine learning\n",
        "- **Multiclass regression** target suitable for academic performance prediction\n",
        "- **Rich graph intelligence** from embeddings and communities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ STEP 5: FINAL DATASET CREATION\n",
            "========================================\n",
            "âœ… Final dataset created: (4102, 149)\n",
            "   Features breakdown:\n",
            "     - Identifiers: 2\n",
            "     - Target (GPA): 1\n",
            "     - Numerical: 11\n",
            "     - One-hot encoded: 7\n",
            "     - Label encoded: 0\n",
            "     - Embeddings: 128\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Final Dataset Creation\n",
        "print(\"ðŸ”„ STEP 5: FINAL DATASET CREATION\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Prepare final feature set\n",
        "numerical_final = [col for col in numerical_features if 'embedding' not in col and col != 'gpa']\n",
        "label_encoded_features = [col for col in df_comprehensive.columns if col.endswith('_encoded')]\n",
        "\n",
        "# Create final dataset\n",
        "df_final = df_comprehensive[['student_id', 'course_id', 'gpa'] + numerical_final + label_encoded_features].copy()\n",
        "\n",
        "# Add one-hot encoded features\n",
        "for feature, encoded_df in encoded_features.items():\n",
        "    df_final = pd.concat([df_final, encoded_df], axis=1)\n",
        "\n",
        "# Add embeddings\n",
        "df_final = pd.concat([df_final, student_emb_df, course_emb_df], axis=1)\n",
        "\n",
        "print(f\"âœ… Final dataset created: {df_final.shape}\")\n",
        "print(f\"   Features breakdown:\")\n",
        "print(f\"     - Identifiers: 2\")\n",
        "print(f\"     - Target (GPA): 1\") \n",
        "print(f\"     - Numerical: {len(numerical_final)}\")\n",
        "print(f\"     - One-hot encoded: {sum(len(enc.columns) for enc in encoded_features.values())}\")\n",
        "print(f\"     - Label encoded: {len(label_encoded_features)}\")\n",
        "print(f\"     - Embeddings: {len(student_emb_df.columns) + len(course_emb_df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸŽ¯ FINAL TRAIN/TEST SPLIT & SCALING\n",
            "==================================================\n",
            "ðŸ“Š Feature matrix: (4102, 146)\n",
            "ðŸ“Š Target vector: (4102,)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input contains NaN",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[113], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Stratified train/test split\u001b[39;00m\n\u001b[1;32m     14\u001b[0m y_binned \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(y, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVery_Low\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedium\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVery_High\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_binned\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Train set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Test set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/DOit_UMBC/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    216\u001b[0m         )\n\u001b[1;32m    217\u001b[0m     ):\n\u001b[0;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    228\u001b[0m     )\n",
            "File \u001b[0;32m~/Desktop/DOit_UMBC/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2940\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2938\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2940\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2942\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2945\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2946\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2947\u001b[0m     )\n\u001b[1;32m   2948\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/DOit_UMBC/.venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2429\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2425\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2427\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   2428\u001b[0m     )\n\u001b[0;32m-> 2429\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
            "File \u001b[0;32m~/Desktop/DOit_UMBC/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1105\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1102\u001b[0m     )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1105\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1114\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/DOit_UMBC/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:105\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
          ]
        }
      ],
      "source": [
        "# Cell 11: Train/Test Split & Scaling\n",
        "print(\"ðŸŽ¯ FINAL TRAIN/TEST SPLIT & SCALING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare features and target\n",
        "feature_columns = [col for col in df_final.columns if col not in ['student_id', 'course_id', 'gpa']]\n",
        "X = df_final[feature_columns]\n",
        "y = df_final['gpa']\n",
        "\n",
        "print(f\"ðŸ“Š Feature matrix: {X.shape}\")\n",
        "print(f\"ðŸ“Š Target vector: {y.shape}\")\n",
        "\n",
        "# Stratified train/test split\n",
        "y_binned = pd.cut(y, bins=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y_binned\n",
        ")\n",
        "\n",
        "print(f\"ðŸ“Š Train set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"ðŸ“Š Test set: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "# Feature scaling (excluding embeddings)\n",
        "embedding_cols = [col for col in X.columns if 'emb_' in col]\n",
        "scaling_cols = [col for col in X.select_dtypes(include=[np.number]).columns if col not in embedding_cols]\n",
        "\n",
        "if len(scaling_cols) > 0:\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "    \n",
        "    X_train_scaled[scaling_cols] = scaler.fit_transform(X_train[scaling_cols])\n",
        "    X_test_scaled[scaling_cols] = scaler.transform(X_test[scaling_cols])\n",
        "    \n",
        "    print(f\"âœ… Applied StandardScaler to {len(scaling_cols)} features\")\n",
        "else:\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "    scaler = None\n",
        "    print(\"â„¹ï¸ No features needed scaling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ SAVING FINAL DATASETS\n",
            "==============================\n",
            "âœ… Saved training set: ../data/train_processed_comprehensive.csv\n",
            "âœ… Saved test set: ../data/test_processed_comprehensive.csv\n",
            "\n",
            "ðŸŽ¯ FEATURE ENGINEERING COMPLETE!\n",
            "ðŸ“Š Dataset Overview:\n",
            "   Original records: 4,072\n",
            "   Final training records: 3,257\n",
            "   Final test records: 815\n",
            "   Total features: 146\n",
            "   Target: Multiclass Regression (GPA 0.0-4.0)\n",
            "\n",
            "ðŸš€ Ready for model training!\n"
          ]
        }
      ],
      "source": [
        "# Cell 12: Save Final Datasets\n",
        "print(\"ðŸ’¾ SAVING FINAL DATASETS\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Create final datasets with identifiers\n",
        "train_final = pd.concat([\n",
        "    df_final.loc[X_train.index, ['student_id', 'course_id']].reset_index(drop=True),\n",
        "    X_train_scaled.reset_index(drop=True),\n",
        "    y_train.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "test_final = pd.concat([\n",
        "    df_final.loc[X_test.index, ['student_id', 'course_id']].reset_index(drop=True),\n",
        "    X_test_scaled.reset_index(drop=True),\n",
        "    y_test.reset_index(drop=True)\n",
        "], axis=1)\n",
        "\n",
        "# Save datasets\n",
        "train_path = \"../data/train_processed_comprehensive.csv\"\n",
        "test_path = \"../data/test_processed_comprehensive.csv\"\n",
        "\n",
        "train_final.to_csv(train_path, index=False)\n",
        "test_final.to_csv(test_path, index=False)\n",
        "\n",
        "print(f\"âœ… Saved training set: {train_path}\")\n",
        "print(f\"âœ… Saved test set: {test_path}\")\n",
        "\n",
        "# Final summary\n",
        "print(f\"\\nðŸŽ¯ FEATURE ENGINEERING COMPLETE!\")\n",
        "print(f\"ðŸ“Š Dataset Overview:\")\n",
        "print(f\"   Original records: {len(df_comprehensive):,}\")\n",
        "print(f\"   Final training records: {len(train_final):,}\")\n",
        "print(f\"   Final test records: {len(test_final):,}\")\n",
        "print(f\"   Total features: {X_train.shape[1]}\")\n",
        "print(f\"   Target: Multiclass Regression (GPA 0.0-4.0)\")\n",
        "print(f\"\\nðŸš€ Ready for model training!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

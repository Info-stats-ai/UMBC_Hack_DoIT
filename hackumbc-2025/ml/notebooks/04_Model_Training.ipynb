{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Academic Risk Prediction - Model Training\n",
        "\n",
        "This notebook implements the machine learning pipeline for predicting academic risk using student data from the UMBC Neo4j graph database.\n",
        "\n",
        "## Steps:\n",
        "1. Load and explore the ML dataset\n",
        "2. Split data into training and testing sets\n",
        "3. Train baseline Logistic Regression model\n",
        "4. Train LightGBM model\n",
        "5. Evaluate and compare models\n",
        "6. Save the best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"LightGBM version: {lgb.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load the ML Dataset\n",
        "\n",
        "Load the `ml_data.csv` file that was created from the Neo4j graph database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the ML dataset\n",
        "try:\n",
        "    df = pd.read_csv('ml_data.csv')\n",
        "    print(\" Successfully loaded ml_data.csv\")\n",
        "    print(f\" Dataset shape: {df.shape}\")\n",
        "    print(f\" Columns: {list(df.columns)}\")\n",
        "    print(\"\\n First few rows:\")\n",
        "    print(df.head())\n",
        "    \n",
        "    print(\"\\n Dataset info:\")\n",
        "    print(df.info())\n",
        "    \n",
        "    print(\"\\n Target variable distribution:\")\n",
        "    if 'academic_risk' in df.columns:\n",
        "        print(df['academic_risk'].value_counts())\n",
        "    else:\n",
        "        print(\"'academic_risk' column not found. Available columns:\")\n",
        "        print(df.columns.tolist())\n",
        "        \n",
        "except FileNotFoundError:\n",
        "    print(\"ml_data.csv not found!\")\n",
        "    print(\"Please make sure the file exists in the current directory.\")\n",
        "    print(\"You may need to run the data extraction notebook first.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preprocessing and Train-Test Split\n",
        "\n",
        "Prepare the data for machine learning by splitting into features and target, then split into 80% training and 20% testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and train-test split\n",
        "def prepare_data(df):\n",
        "    \"\"\"\n",
        "    Prepare data for machine learning\n",
        "    \"\"\"\n",
        "    # Identify target variable (assuming it's 'academic_risk' or similar)\n",
        "    target_columns = ['academic_risk', 'risk_level', 'at_risk', 'target']\n",
        "    target_col = None\n",
        "    \n",
        "    for col in target_columns:\n",
        "        if col in df.columns:\n",
        "            target_col = col\n",
        "            break\n",
        "    \n",
        "    if target_col is None:\n",
        "        print(\"No target column found. Please specify the target column name.\")\n",
        "        return None, None, None, None\n",
        "    \n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "    \n",
        "    # Handle categorical variables (if any)\n",
        "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
        "    if len(categorical_columns) > 0:\n",
        "        print(f\" Found categorical columns: {list(categorical_columns)}\")\n",
        "        # For now, we'll use label encoding (you can improve this later)\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        le = LabelEncoder()\n",
        "        for col in categorical_columns:\n",
        "            X[col] = le.fit_transform(X[col].astype(str))\n",
        "    \n",
        "    # Split the data into 80% training and 20% testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    print(f\" Data split completed:\")\n",
        "    print(f\"   Training set: {X_train.shape[0]} samples\")\n",
        "    print(f\"   Testing set: {X_test.shape[0]} samples\")\n",
        "    print(f\"   Features: {X_train.shape[1]}\")\n",
        "    print(f\"   Target distribution in training set:\")\n",
        "    print(f\"   {y_train.value_counts()}\")\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Prepare the data\n",
        "X_train, X_test, y_train, y_test = prepare_data(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Baseline Logistic Regression Model\n",
        "\n",
        "Train a baseline Logistic Regression model and evaluate its F1-score on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline Logistic Regression model\n",
        "def train_baseline_model(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate a baseline Logistic Regression model\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Training baseline Logistic Regression model...\")\n",
        "    \n",
        "    # Initialize and train the model\n",
        "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    lr_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_lr = lr_model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
        "    precision_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
        "    recall_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
        "    \n",
        "    print(\" Baseline Logistic Regression Results:\")\n",
        "    print(f\"   F1-Score: {f1_lr:.4f}\")\n",
        "    print(f\"   Precision: {precision_lr:.4f}\")\n",
        "    print(f\"   Recall: {recall_lr:.4f}\")\n",
        "    \n",
        "    # Display classification report\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_lr))\n",
        "    \n",
        "    # Display confusion matrix\n",
        "    print(\"\\n Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred_lr)\n",
        "    print(cm)\n",
        "    \n",
        "    return lr_model, f1_lr\n",
        "\n",
        "# Train the baseline model\n",
        "if X_train is not None:\n",
        "    lr_model, baseline_f1 = train_baseline_model(X_train, X_test, y_train, y_test)\n",
        "else:\n",
        "    print(\" Cannot train model - data preparation failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train LightGBM Model\n",
        "\n",
        "Train the main LightGBM model on the same training data and evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LightGBM model\n",
        "def train_lightgbm_model(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate a LightGBM model\n",
        "    \"\"\"\n",
        "    print(\" Training LightGBM model...\")\n",
        "    \n",
        "    # LightGBM parameters\n",
        "    lgb_params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': len(np.unique(y_train)),\n",
        "        'metric': 'multi_logloss',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': 31,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 5,\n",
        "        'verbose': -1,\n",
        "        'random_state': 42\n",
        "    }\n",
        "    \n",
        "    # Create LightGBM datasets\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
        "    \n",
        "    # Train the model\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params,\n",
        "        train_data,\n",
        "        valid_sets=[test_data],\n",
        "        num_boost_round=1000,\n",
        "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)]\n",
        "    )\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
        "    y_pred_lgb = np.argmax(y_pred_lgb, axis=1)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    f1_lgb = f1_score(y_test, y_pred_lgb, average='weighted')\n",
        "    precision_lgb = precision_score(y_test, y_pred_lgb, average='weighted')\n",
        "    recall_lgb = recall_score(y_test, y_pred_lgb, average='weighted')\n",
        "    \n",
        "    print(\" LightGBM Results:\")\n",
        "    print(f\"   F1-Score: {f1_lgb:.4f}\")\n",
        "    print(f\"   Precision: {precision_lgb:.4f}\")\n",
        "    print(f\"   Recall: {recall_lgb:.4f}\")\n",
        "    print(f\"   Best iteration: {lgb_model.best_iteration}\")\n",
        "    \n",
        "    # Display classification report\n",
        "    print(\"\\n Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_lgb))\n",
        "    \n",
        "    # Display confusion matrix\n",
        "    print(\"\\n Confusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred_lgb)\n",
        "    print(cm)\n",
        "    \n",
        "    return lgb_model, f1_lgb, precision_lgb, recall_lgb\n",
        "\n",
        "# Train the LightGBM model\n",
        "if X_train is not None:\n",
        "    lgb_model, lgb_f1, lgb_precision, lgb_recall = train_lightgbm_model(X_train, X_test, y_train, y_test)\n",
        "else:\n",
        "    print(\"Cannot train model - data preparation failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Model Comparison and Feature Importance\n",
        "\n",
        "Compare the performance of both models and analyze feature importance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison and feature importance\n",
        "def compare_models(baseline_f1, lgb_f1, lgb_precision, lgb_recall):\n",
        "    \"\"\"\n",
        "    Compare model performance and display results\n",
        "    \"\"\"\n",
        "    print(\"Model Comparison Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Baseline Logistic Regression F1-Score: {baseline_f1:.4f}\")\n",
        "    print(f\"LightGBM F1-Score:                    {lgb_f1:.4f}\")\n",
        "    print(f\"LightGBM Precision:                   {lgb_precision:.4f}\")\n",
        "    print(f\"LightGBM Recall:                      {lgb_recall:.4f}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    improvement = lgb_f1 - baseline_f1\n",
        "    if improvement > 0:\n",
        "        print(f\"LightGBM improves F1-score by {improvement:.4f}\")\n",
        "    else:\n",
        "        print(f\"LightGBM F1-score is {abs(improvement):.4f} lower than baseline\")\n",
        "\n",
        "def plot_feature_importance(lgb_model, feature_names, top_n=20):\n",
        "    \"\"\"\n",
        "    Plot feature importance from LightGBM model\n",
        "    \"\"\"\n",
        "    # Get feature importance\n",
        "    importance = lgb_model.feature_importance(importance_type='gain')\n",
        "    \n",
        "    # Create DataFrame\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    # Plot top features\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    top_features = feature_importance_df.head(top_n)\n",
        "    sns.barplot(data=top_features, x='importance', y='feature')\n",
        "    plt.title(f'Top {top_n} Feature Importance (LightGBM)')\n",
        "    plt.xlabel('Importance (Gain)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return feature_importance_df\n",
        "\n",
        "# Compare models\n",
        "if 'baseline_f1' in locals() and 'lgb_f1' in locals():\n",
        "    compare_models(baseline_f1, lgb_f1, lgb_precision, lgb_recall)\n",
        "    \n",
        "    # Plot feature importance\n",
        "    if X_train is not None:\n",
        "        feature_importance_df = plot_feature_importance(lgb_model, X_train.columns)\n",
        "        print(\"\\nüîç Top 10 Most Important Features:\")\n",
        "        print(feature_importance_df.head(10)[['feature', 'importance']])\n",
        "else:\n",
        "    print(\" Cannot compare models - training failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save the Trained LightGBM Model\n",
        "\n",
        "Save the trained LightGBM model to a file named `academic_risk_model.joblib` using the joblib library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained LightGBM model using joblib\n",
        "def save_model(lgb_model, filename='academic_risk_model.joblib'):\n",
        "    \"\"\"\n",
        "    Save the trained LightGBM model using joblib\n",
        "    \"\"\"\n",
        "    try:\n",
        "        joblib.dump(lgb_model, filename)\n",
        "        print(f\" Model successfully saved to {filename}\")\n",
        "        \n",
        "        # Verify the model can be loaded\n",
        "        loaded_model = joblib.load(filename)\n",
        "        print(f\" Model verification successful - can be loaded from {filename}\")\n",
        "        \n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\" Error saving model: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Save the model\n",
        "if 'lgb_model' in locals():\n",
        "    save_model(lgb_model, 'academic_risk_model.joblib')\n",
        "else:\n",
        "    print(\" No trained model found to save\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

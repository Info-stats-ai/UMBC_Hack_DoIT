{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057c3d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "import pandas as pd\n",
    "\n",
    "NEO4J_URI = \"bolt://127.0.0.1:7687\"  # use direct bolt to avoid routing issues\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"Iwin@27100\"\n",
    "NEO4J_DB = \"smalldata\"\n",
    "GDS_GRAPH_NAME = \"umbc_graph\"\n",
    "\n",
    "# Reuse driver if available; verify and recreate if broken\n",
    "try:\n",
    "    driver\n",
    "    try:\n",
    "        driver.verify_connectivity()\n",
    "    except Exception:\n",
    "        try:\n",
    "            driver.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "except NameError:\n",
    "    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71144fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDS available: True\n"
     ]
    }
   ],
   "source": [
    "# Check if Graph Data Science (GDS) plugin is available (Neo4j 5+)\n",
    "with driver.session(database=NEO4J_DB) as session:\n",
    "    try:\n",
    "        proc_names = session.run(\"SHOW PROCEDURES YIELD name RETURN name\").value()\n",
    "    except Exception:\n",
    "        proc_names = []\n",
    "\n",
    "have_gds = any(str(n).startswith(\"gds.\") for n in proc_names)\n",
    "\n",
    "# Direct probe for GDS version (more reliable)\n",
    "if not have_gds:\n",
    "    with driver.session(database=NEO4J_DB) as session:\n",
    "        try:\n",
    "            _ = session.run(\"CALL gds.version() YIELD version RETURN version\").single()\n",
    "            have_gds = True\n",
    "        except Exception:\n",
    "            have_gds = False\n",
    "\n",
    "print(\"GDS available:\", have_gds)\n",
    "\n",
    "if not have_gds:\n",
    "    print(\n",
    "        \"Graph Data Science is not installed/enabled. In Neo4j Desktop: Manage > Plugins > install 'Graph Data Science', then restart the DB.\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e91a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph projection ensured.\n"
     ]
    }
   ],
   "source": [
    "# Ensure GDS projection exists without APOC dependency\n",
    "from neo4j.exceptions import ClientError\n",
    "\n",
    "if have_gds:\n",
    "    with driver.session(database=NEO4J_DB) as session:\n",
    "        exists = session.run(\"CALL gds.graph.exists($name) YIELD exists RETURN exists\", {\"name\": GDS_GRAPH_NAME}).single()[\"exists\"]\n",
    "        if not exists:\n",
    "            session.run(f\"\"\"\n",
    "            CALL gds.graph.project('{GDS_GRAPH_NAME}',\n",
    "              ['Student','Course'],\n",
    "              {{\n",
    "                COMPLETED: {{type: 'COMPLETED', orientation: 'UNDIRECTED'}},\n",
    "                ENROLLED_IN: {{type: 'ENROLLED_IN', orientation: 'UNDIRECTED'}}\n",
    "              }})\n",
    "            \"\"\")\n",
    "    print(\"Graph projection ensured.\")\n",
    "else:\n",
    "    print(\"Skipping projection: GDS plugin not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39c1e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastRP embeddings and Louvain community written to nodes.\n"
     ]
    }
   ],
   "source": [
    "# Run FastRP and Louvain and write properties to DB\n",
    "if have_gds:\n",
    "    with driver.session(database=NEO4J_DB) as session:\n",
    "        session.run(f\"CALL gds.fastRP.write('{GDS_GRAPH_NAME}', {{ writeProperty: 'fastRP_embedding', embeddingDimension: 32 }})\")\n",
    "        session.run(f\"CALL gds.louvain.write('{GDS_GRAPH_NAME}', {{ writeProperty: 'louvain_community' }})\")\n",
    "    print(\"FastRP embeddings and Louvain community written to nodes.\")\n",
    "else:\n",
    "    print(\"Skipping GDS algorithms: GDS plugin not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d511b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25 rows to ../data/ml_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels into DataFrame and save to CSV\n",
    "import numpy as np\n",
    "\n",
    "def grade_to_label(g):\n",
    "    if g is None:\n",
    "        return None\n",
    "    g = str(g).strip().upper()\n",
    "    return 1 if g.startswith(('A','B')) else 0\n",
    "\n",
    "with driver.session(database=NEO4J_DB) as session:\n",
    "    query = \"\"\"\n",
    "    MATCH (s:Student)-[r:COMPLETED]->(c:Course)\n",
    "    RETURN s.id AS student_id,\n",
    "           c.id AS course_id,\n",
    "           r.grade AS grade,\n",
    "           s.fastRP_embedding AS s_emb,\n",
    "           c.fastRP_embedding AS c_emb,\n",
    "           s.louvain_community AS s_comm,\n",
    "           c.louvain_community AS c_comm\n",
    "    \"\"\"\n",
    "    rows = session.run(query).data()\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Map grade to binary label\n",
    "df['label'] = df['grade'].apply(grade_to_label)\n",
    "\n",
    "# Expand embeddings into columns\n",
    "s_emb_df = pd.DataFrame(df['s_emb'].tolist()).add_prefix('s_emb_')\n",
    "c_emb_df = pd.DataFrame(df['c_emb'].tolist()).add_prefix('c_emb_')\n",
    "\n",
    "# Concatenate final dataset\n",
    "out_df = pd.concat([\n",
    "    df[['student_id','course_id','s_comm','c_comm','label']],\n",
    "    s_emb_df, c_emb_df\n",
    "], axis=1)\n",
    "\n",
    "csv_path = os.path.join(\"../data\", \"ml_data.csv\")\n",
    "out_df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved {len(out_df)} rows to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a93a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c527ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e988df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Overview ===\n",
      "Rows: 25\n",
      "Columns: 69\n",
      "Columns: ['student_id', 'course_id', 's_comm', 'c_comm', 'label', 's_emb_0', 's_emb_1', 's_emb_2', 's_emb_3', 's_emb_4', 's_emb_5', 's_emb_6', 's_emb_7', 's_emb_8', 's_emb_9', 's_emb_10', 's_emb_11', 's_emb_12', 's_emb_13', 's_emb_14', 's_emb_15', 's_emb_16', 's_emb_17', 's_emb_18', 's_emb_19', 's_emb_20', 's_emb_21', 's_emb_22', 's_emb_23', 's_emb_24', 's_emb_25', 's_emb_26', 's_emb_27', 's_emb_28', 's_emb_29', 's_emb_30', 's_emb_31', 'c_emb_0', 'c_emb_1', 'c_emb_2', 'c_emb_3', 'c_emb_4', 'c_emb_5', 'c_emb_6', 'c_emb_7', 'c_emb_8', 'c_emb_9', 'c_emb_10', 'c_emb_11', 'c_emb_12', 'c_emb_13', 'c_emb_14', 'c_emb_15', 'c_emb_16', 'c_emb_17', 'c_emb_18', 'c_emb_19', 'c_emb_20', 'c_emb_21', 'c_emb_22', 'c_emb_23', 'c_emb_24', 'c_emb_25', 'c_emb_26', 'c_emb_27', 'c_emb_28', 'c_emb_29', 'c_emb_30', 'c_emb_31']\n",
      "\n",
      "Head:\n",
      "  student_id   course_id  s_comm  c_comm  label   s_emb_0   s_emb_1   s_emb_2  \\\n",
      "0    ZO28124    CSEE 200      47      39      1  0.209518  0.107594  0.162222   \n",
      "1    ZO28124  CSLL 100-6      47      47      0  0.209518  0.107594  0.162222   \n",
      "2    ZO28124    BUUU 100      47      47      1  0.209518  0.107594  0.162222   \n",
      "\n",
      "    s_emb_3   s_emb_4  ...  c_emb_22  c_emb_23  c_emb_24  c_emb_25  c_emb_26  \\\n",
      "0  0.564889 -0.041032  ... -0.399910  0.067994 -0.012444 -0.038177  0.566134   \n",
      "1  0.564889 -0.041032  ... -0.340937  0.169547  0.113663  0.023940  0.285616   \n",
      "2  0.564889 -0.041032  ... -0.485063  0.193390  0.083078 -0.012971  0.349205   \n",
      "\n",
      "   c_emb_27  c_emb_28  c_emb_29  c_emb_30  c_emb_31  \n",
      "0 -0.335569  0.559753  0.168958 -0.254097 -0.122800  \n",
      "1 -0.179411  0.340937  0.171953  0.127143 -0.044844  \n",
      "2 -0.351396  0.269064  0.040524  0.118161 -0.174008  \n",
      "\n",
      "[3 rows x 69 columns]\n",
      "\n",
      "Missing per column:\n",
      "student_id    0\n",
      "c_emb_7       0\n",
      "c_emb_13      0\n",
      "c_emb_12      0\n",
      "c_emb_11      0\n",
      "c_emb_10      0\n",
      "c_emb_9       0\n",
      "c_emb_8       0\n",
      "c_emb_6       0\n",
      "s_emb_30      0\n",
      "c_emb_5       0\n",
      "c_emb_4       0\n",
      "c_emb_3       0\n",
      "c_emb_2       0\n",
      "c_emb_1       0\n",
      "c_emb_0       0\n",
      "c_emb_14      0\n",
      "c_emb_15      0\n",
      "c_emb_16      0\n",
      "c_emb_17      0\n",
      "dtype: int64\n",
      "\n",
      "Dropped duplicates: 0\n",
      "\n",
      "Train rows: 20, Test rows: 5\n",
      "\n",
      "Dropping 30 highly correlated columns (>|0.8|)\n",
      "\n",
      "=== Post-Engineering Summary (Train) ===\n",
      "Rows: 20, Cols: 39\n",
      "Columns: ['student_id', 'course_id', 's_comm', 'c_comm', 'label', 's_emb_0', 's_emb_1', 's_emb_2', 's_emb_4', 's_emb_5', 's_emb_6', 's_emb_7', 's_emb_8', 's_emb_9', 's_emb_11', 's_emb_12', 's_emb_13', 's_emb_17', 's_emb_18', 's_emb_20'] ...\n",
      "Label distribution (train):\n",
      "label\n",
      "1    0.65\n",
      "0    0.35\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Post-Engineering Summary (Test) ===\n",
      "Rows: 5, Cols: 39\n",
      "Saved train to ../data/train_processed.csv\n",
      "Saved test to ../data/test_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering Pipeline on ml_data.csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "DATA_PATH = os.path.join(\"../data\", \"ml_data.csv\")\n",
    "PROCESSED_DIR = \"../data\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load\n",
    "raw = pd.read_csv(DATA_PATH)\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Rows: {len(raw):,}\")\n",
    "print(f\"Columns: {len(raw.columns):,}\")\n",
    "print(\"Columns:\", list(raw.columns))\n",
    "print(\"\\nHead:\")\n",
    "print(raw.head(3))\n",
    "print(\"\\nMissing per column:\")\n",
    "print(raw.isna().sum().sort_values(ascending=False).head(20))\n",
    "\n",
    "# 2) Remove duplicates\n",
    "before = len(raw)\n",
    "raw = raw.drop_duplicates()\n",
    "print(f\"\\nDropped duplicates: {before - len(raw)}\")\n",
    "\n",
    "# 3) Basic NA handling: keep id cols; for features, fill numeric with median\n",
    "id_cols = [c for c in raw.columns if c in (\"student_id\",\"course_id\")]\n",
    "label_col = \"label\" if \"label\" in raw.columns else None\n",
    "feature_cols = [c for c in raw.columns if c not in id_cols + ([label_col] if label_col else [])]\n",
    "\n",
    "numeric_cols = [c for c in feature_cols if np.issubdtype(raw[c].dropna().dtype, np.number)]\n",
    "object_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "raw[numeric_cols] = raw[numeric_cols].fillna(raw[numeric_cols].median())\n",
    "\n",
    "# 4) Split train/test (stratify on label if present)\n",
    "if label_col:\n",
    "    y = raw[label_col].values\n",
    "    strat = y if len(np.unique(y)) > 1 else None\n",
    "else:\n",
    "    y = None\n",
    "    strat = None\n",
    "\n",
    "train_df, test_df = train_test_split(raw, test_size=0.2, random_state=42, stratify=strat)\n",
    "print(f\"\\nTrain rows: {len(train_df):,}, Test rows: {len(test_df):,}\")\n",
    "\n",
    "# 5) Encoding\n",
    "# - Nominal (low-cardinality) -> One-Hot\n",
    "# - Cardinal (high-cardinality) -> Frequency encoding\n",
    "\n",
    "def split_nominal_cardinal(series: pd.Series, cardinality_threshold: int = 20):\n",
    "    uniq = series.dropna().unique()\n",
    "    return (\"nominal\" if len(uniq) <= cardinality_threshold else \"cardinal\")\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "# Determine categorical columns on train only\n",
    "categorical_cols = [c for c in object_cols if c in train_df.columns]\n",
    "nominal_cols = [c for c in categorical_cols if split_nominal_cardinal(train_df[c]) == \"nominal\"]\n",
    "cardinal_cols = [c for c in categorical_cols if c not in nominal_cols]\n",
    "\n",
    "# One-hot for nominal\n",
    "if nominal_cols:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    ohe.fit(train_df[nominal_cols])\n",
    "    encoders[\"ohe\"] = (ohe, nominal_cols)\n",
    "\n",
    "# Frequency encoding for cardinal\n",
    "freq_maps = {}\n",
    "for c in cardinal_cols:\n",
    "    freq = train_df[c].value_counts(dropna=False)\n",
    "    freq_maps[c] = freq / freq.sum()\n",
    "encoders[\"freq\"] = freq_maps\n",
    "\n",
    "# Apply encodings\n",
    "def apply_encodings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Frequency encodings\n",
    "    for c, fmap in encoders.get(\"freq\", {}).items():\n",
    "        out[f\"{c}_freq\"] = out[c].map(fmap).fillna(0)\n",
    "    # OHE\n",
    "    if \"ohe\" in encoders:\n",
    "        ohe, cols = encoders[\"ohe\"]\n",
    "        ohe_arr = ohe.transform(out[cols])\n",
    "        ohe_cols = ohe.get_feature_names_out(cols)\n",
    "        ohe_df = pd.DataFrame(ohe_arr, columns=ohe_cols, index=out.index)\n",
    "        out = pd.concat([out.drop(columns=cols), ohe_df], axis=1)\n",
    "    return out\n",
    "\n",
    "train_fe = apply_encodings(train_df)\n",
    "test_fe = apply_encodings(test_df)\n",
    "\n",
    "# 6) Drop highly correlated numeric features (>|0.8|) based on train only\n",
    "corr_threshold = 0.8\n",
    "num_cols_train = [c for c in train_fe.columns if c not in id_cols + ([label_col] if label_col else []) and np.issubdtype(train_fe[c].dtype, np.number)]\n",
    "\n",
    "corr = train_fe[num_cols_train].corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "cols_to_drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "print(f\"\\nDropping {len(cols_to_drop)} highly correlated columns (>|{corr_threshold}|)\")\n",
    "\n",
    "train_fe = train_fe.drop(columns=cols_to_drop)\n",
    "test_fe = test_fe.drop(columns=[c for c in cols_to_drop if c in test_fe.columns])\n",
    "\n",
    "# 7) Final reports\n",
    "print(\"\\n=== Post-Engineering Summary (Train) ===\")\n",
    "print(f\"Rows: {len(train_fe):,}, Cols: {len(train_fe.columns):,}\")\n",
    "print(\"Columns:\", list(train_fe.columns)[:20], (\"...\" if len(train_fe.columns) > 20 else \"\"))\n",
    "print(\"Label distribution (train):\" if label_col else \"No label column found\")\n",
    "if label_col:\n",
    "    print(train_fe[label_col].value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"\\n=== Post-Engineering Summary (Test) ===\")\n",
    "print(f\"Rows: {len(test_fe):,}, Cols: {len(test_fe.columns):,}\")\n",
    "\n",
    "# 8) Save processed splits\n",
    "train_out = os.path.join(PROCESSED_DIR, \"train_processed.csv\")\n",
    "test_out = os.path.join(PROCESSED_DIR, \"test_processed.csv\")\n",
    "train_fe.to_csv(train_out, index=False)\n",
    "test_fe.to_csv(test_out, index=False)\n",
    "print(f\"Saved train to {train_out}\")\n",
    "print(f\"Saved test to {test_out}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
